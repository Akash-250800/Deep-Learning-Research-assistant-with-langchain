{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "412d7376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579e735b",
   "metadata": {},
   "source": [
    "Load the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73c060c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'arxiv-metadata-oai-snapshot.json'\n",
    "limit = 10000   # Loading only 10,000 papers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538244b7",
   "metadata": {},
   "source": [
    "Targeting specific categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ccca0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_categories = ['cs.AI', 'cs.LG', 'cs.CL', 'cs.CV']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e3d9eb",
   "metadata": {},
   "source": [
    "Combining title and abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c18e96",
   "metadata": {},
   "source": [
    "Checking the target category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16fa974a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        paper = json.loads(line)\n",
    "        categories = paper.get('categories', '')\n",
    "\n",
    "        # Checking if any target category is in the paper's categories\n",
    "        if any(cat in categories.split() for cat in target_categories):\n",
    "            data.append(paper)\n",
    "\n",
    "\n",
    "        if len(data) >= limit:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7193faf7",
   "metadata": {},
   "source": [
    "Building DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aef1bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60a903d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['title'] + '. ' + df['abstract']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48705e76",
   "metadata": {},
   "source": [
    " Drop empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e198893",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['content'].notna()]\n",
    "df = df[df['content'].str.strip() != '']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6c71cd",
   "metadata": {},
   "source": [
    "Final document list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2bb7764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 documents\n",
      "Intelligent location of simultaneously active acoustic emission sources:\n",
      "  Part I.   The intelligent acoustic emission locator is described in Part I, while Part\n",
      "II discusses blind source separation, time delay estimation and location of two\n",
      "simultaneously active continuous acoustic emission sources.\n",
      "  The location of acoustic emission on complicated aircraft frame structures is\n",
      "a difficult problem of non-destructive testing. This article describes an\n",
      "intelligent acoustic emission source locator. The intelligent locator comprises\n",
      "a sensor antenna and a general regression neural network, which solves the\n",
      "location problem based on learning from examples. Locator performance was\n",
      "tested on different test specimens. Tests have shown that the accuracy of\n",
      "location depends on sound velocity and attenuation in the specimen, the\n",
      "dimensions of the tested area, and the properties of stored data. The location\n",
      "accuracy achieved by the intelligent locator is comparable to that obtained by\n",
      "the conventional triangulation method, while the applicability of the\n",
      "intelligent locator is more general since analysis of sonic ray paths is\n",
      "avoided. This is a promising method for non-destructive testing of aircraft\n",
      "frame structures by the acoustic emission method.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documents = df['content'].tolist()\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "print(documents[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d288866d",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97a324ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Assistant\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import InputExample\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "943cf29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21b0fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building positive Examples\n",
    "for doc in documents:\n",
    "    train_examples.append(InputExample(texts=[doc, doc], label=1.0))  # Positive pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79b84280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building negative Examples\n",
    "for _ in range(len(documents) // 2):  # Half number of negatives\n",
    "    doc1 = random.choice(documents)\n",
    "    doc2 = random.choice(documents)\n",
    "    if doc1 != doc2:  # Avoid same doc\n",
    "        train_examples.append(InputExample(texts=[doc1, doc2], label=0.0))  # Negative pair\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b890a12",
   "metadata": {},
   "source": [
    "Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "868251e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, losses\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca20e19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a base pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67085d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare DataLoader\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "969ebebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "382db773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Assistant\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='938' max='938' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [938/938 2:01:26, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.004400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fine-tune\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=1,\n",
    "    warmup_steps=100,\n",
    "    output_path='./fine_tuned_model'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f27127",
   "metadata": {},
   "source": [
    "Building FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fad82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9ed779",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('./fine_tuned_model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6a2fb5",
   "metadata": {},
   "source": [
    "Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab9f87a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 313/313 [05:20<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.encode(documents, device='cpu', show_progress_bar=True, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fb02e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert embeddings to numpy array\n",
    "embeddings = np.array(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc586e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating FAISS index\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb012776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS database built and saved!\n"
     ]
    }
   ],
   "source": [
    "# Saving FAISS index and documents\n",
    "faiss.write_index(index, \"faiss_index.bin\")\n",
    "with open('documents.pkl', 'wb') as f:\n",
    "    pickle.dump(documents, f)\n",
    "\n",
    "print(\"FAISS database built and saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae850f52",
   "metadata": {},
   "source": [
    "LangChain connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3218dd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "import pickle\n",
    "import faiss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e46db0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents\n",
    "with open('documents.pkl', 'rb') as f:\n",
    "    documents = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08759ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FAISS index\n",
    "index = faiss.read_index('faiss_index.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bec0dc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akash\\AppData\\Local\\Temp\\ipykernel_16900\\1133240518.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"./fine_tuned_model\")\n"
     ]
    }
   ],
   "source": [
    "# Load embedding model (fine-tuned one)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"./fine_tuned_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae124cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LangChain FAISS store\n",
    "vectorstore = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(dict(enumerate(documents))),\n",
    "    index_to_docstore_id=lambda i: i\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
