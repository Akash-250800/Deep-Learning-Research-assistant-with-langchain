---
tags:
- sentence-transformers
- sentence-similarity
- feature-extraction
- generated_from_trainer
- dataset_size:14998
- loss:CosineSimilarityLoss
base_model: sentence-transformers/all-MiniLM-L6-v2
widget:
- source_sentence: 'An Adaptive Algorithm for Finite Stochastic Partial Monitoring.   We
    present a new anytime algorithm that achieves near-optimal regret for any

    instance of finite stochastic partial monitoring. In particular, the new

    algorithm achieves the minimax regret, within logarithmic factors, for both

    "easy" and "hard" problems. For easy problems, it additionally achieves

    logarithmic individual regret. Most importantly, the algorithm is adaptive in

    the sense that if the opponent strategy is in an "easy region" of the strategy

    space then the regret grows as if the problem was easy. As an implication, we

    show that under some reasonable additional assumptions, the algorithm enjoys an

    O(\sqrt{T}) regret in Dynamic Pricing, proven to be hard by Bartok et al.

    (2011).

    '
  sentences:
  - 'An Adaptive Algorithm for Finite Stochastic Partial Monitoring.   We present
    a new anytime algorithm that achieves near-optimal regret for any

    instance of finite stochastic partial monitoring. In particular, the new

    algorithm achieves the minimax regret, within logarithmic factors, for both

    "easy" and "hard" problems. For easy problems, it additionally achieves

    logarithmic individual regret. Most importantly, the algorithm is adaptive in

    the sense that if the opponent strategy is in an "easy region" of the strategy

    space then the regret grows as if the problem was easy. As an implication, we

    show that under some reasonable additional assumptions, the algorithm enjoys an

    O(\sqrt{T}) regret in Dynamic Pricing, proven to be hard by Bartok et al.

    (2011).

    '
  - 'Hilbert Space Embeddings of POMDPs.   A nonparametric approach for policy learning
    for POMDPs is proposed. The

    approach represents distributions over the states, observations, and actions as

    embeddings in feature spaces, which are reproducing kernel Hilbert spaces.

    Distributions over states given the observations are obtained by applying the

    kernel Bayes'' rule to these distribution embeddings. Policies and value

    functions are defined on the feature space over states, which leads to a

    feature space expression for the Bellman equation. Value iteration may then be

    used to estimate the optimal value function and associated policy. Experimental

    results confirm that the correct policy is learned using the feature space

    representation.

    '
  - 'A Compact Self-organizing Cellular Automata-based Genetic Algorithm.   A Genetic
    Algorithm (GA) is proposed in which each member of the population

    can change schemata only with its neighbors according to a rule. The rule

    methodology and the neighborhood structure employ elements from the Cellular

    Automata (CA) strategies. Each member of the GA population is assigned to a

    cell and crossover takes place only between adjacent cells, according to the

    predefined rule. Although combinations of CA and GA approaches have appeared

    previously, here we rely on the inherent self-organizing features of CA, rather

    than on parallelism. This conceptual shift directs us toward the evolution of

    compact populations containing only a handful of members. We find that the

    resulting algorithm can search the design space more efficiently than

    traditional GA strategies due to its ability to exploit mutations within this

    compact self-organizing population. Consequently, premature convergence is

    avoided and the final results often are more accurate. In order to reinforce

    the superior mutation capability, a re-initialization strategy also is

    implemented. Ten test functions and two benchmark structural engineering truss

    design problems are examined in order to demonstrate the performance of the

    method.

    '
- source_sentence: 'Improving MUC extraction thanks to local search.   ExtractingMUCs(MinimalUnsatisfiableCores)fromanunsatisfiable
    constraint

    network is a useful process when causes of unsatisfiability must be understood

    so that the network can be re-engineered and relaxed to become sat- isfiable.

    Despite bad worst-case computational complexity results, various MUC- finding

    approaches that appear tractable for many real-life instances have been

    proposed. Many of them are based on the successive identification of so-called

    transition constraints. In this respect, we show how local search can be used

    to possibly extract additional transition constraints at each main iteration

    step. The approach is shown to outperform a technique based on a form of model

    rotation imported from the SAT-related technology and that also exhibits

    additional transi- tion constraints. Our extensive computational

    experimentations show that this en- hancement also boosts the performance of

    state-of-the-art DC(WCORE)-like MUC extractors.

    '
  sentences:
  - "Dependence Maximizing Temporal Alignment via Squared-Loss Mutual\n  Information.\
    \   The goal of temporal alignment is to establish time correspondence between\n\
    two sequences, which has many applications in a variety of areas such as speech\n\
    processing, bioinformatics, computer vision, and computer graphics. In this\n\
    paper, we propose a novel temporal alignment method called least-squares\ndynamic\
    \ time warping (LSDTW). LSDTW finds an alignment that maximizes\nstatistical dependency\
    \ between sequences, measured by a squared-loss variant of\nmutual information.\
    \ The benefit of this novel information-theoretic formulation\nis that LSDTW can\
    \ align sequences with different lengths, different\ndimensionality, high non-linearity,\
    \ and non-Gaussianity in a computationally\nefficient manner. In addition, model\
    \ parameters such as an initial alignment\nmatrix can be systematically optimized\
    \ by cross-validation. We demonstrate the\nusefulness of LSDTW through experiments\
    \ on synthetic and real-world Kinect\naction recognition datasets.\n"
  - 'Clustering of Local Optima in Combinatorial Fitness Landscapes.   Using the recently
    proposed model of combinatorial landscapes: local optima

    networks, we study the distribution of local optima in two classes of instances

    of the quadratic assignment problem. Our results indicate that the two problem

    instance classes give rise to very different configuration spaces. For the

    so-called real-like class, the optima networks possess a clear modular

    structure, while the networks belonging to the class of random uniform

    instances are less well partitionable into clusters. We briefly discuss the

    consequences of the findings for heuristically searching the corresponding

    problem spaces.

    '
  - 'Improving MUC extraction thanks to local search.   ExtractingMUCs(MinimalUnsatisfiableCores)fromanunsatisfiable
    constraint

    network is a useful process when causes of unsatisfiability must be understood

    so that the network can be re-engineered and relaxed to become sat- isfiable.

    Despite bad worst-case computational complexity results, various MUC- finding

    approaches that appear tractable for many real-life instances have been

    proposed. Many of them are based on the successive identification of so-called

    transition constraints. In this respect, we show how local search can be used

    to possibly extract additional transition constraints at each main iteration

    step. The approach is shown to outperform a technique based on a form of model

    rotation imported from the SAT-related technology and that also exhibits

    additional transi- tion constraints. Our extensive computational

    experimentations show that this en- hancement also boosts the performance of

    state-of-the-art DC(WCORE)-like MUC extractors.

    '
- source_sentence: 'Syntactic variation of support verb constructions.   We report
    experiments about the syntactic variations of support verb

    constructions, a special type of multiword expressions (MWEs) containing

    predicative nouns. In these expressions, the noun can occur with or without the

    verb, with no clear-cut semantic difference. We extracted from a large French

    corpus a set of examples of the two situations and derived statistical results

    from these data. The extraction involved large-coverage language resources and

    finite-state techniques. The results show that, most frequently, predicative

    nouns occur without a support verb. This fact has consequences on methods of

    extracting or recognising MWEs.

    '
  sentences:
  - "Use of statistical outlier detection method in adaptive evolutionary\n  algorithms.\
    \   In this paper, the issue of adapting probabilities for Evolutionary Algorithm\n\
    (EA) search operators is revisited. A framework is devised for distinguishing\n\
    between measurements of performance and the interpretation of those\nmeasurements\
    \ for purposes of adaptation. Several examples of measurements and\nstatistical\
    \ interpretations are provided. Probability value adaptation is\ntested using\
    \ an EA with 10 search operators against 10 test problems with\nresults indicating\
    \ that both the type of measurement and its statistical\ninterpretation play significant\
    \ roles in EA performance. We also find that\nselecting operators based on the\
    \ prevalence of outliers rather than on average\nperformance is able to provide\
    \ considerable improvements to adaptive methods\nand soundly outperforms the non-adaptive\
    \ case.\n"
  - 'Bayesian Optimization in a Billion Dimensions via Random Embeddings.   Bayesian
    optimization techniques have been successfully applied to robotics,

    planning, sensor placement, recommendation, advertising, intelligent user

    interfaces and automatic algorithm configuration. Despite these successes, the

    approach is restricted to problems of moderate dimension, and several workshops

    on Bayesian optimization have identified its scaling to high-dimensions as one

    of the holy grails of the field. In this paper, we introduce a novel random

    embedding idea to attack this problem. The resulting Random EMbedding Bayesian

    Optimization (REMBO) algorithm is very simple, has important invariance

    properties, and applies to domains with both categorical and continuous

    variables. We present a thorough theoretical analysis of REMBO. Empirical

    results confirm that REMBO can effectively solve problems with billions of

    dimensions, provided the intrinsic dimensionality is low. They also show that

    REMBO achieves state-of-the-art performance in optimizing the 47 discrete

    parameters of a popular mixed integer linear programming solver.

    '
  - "h-approximation: History-Based Approximation of Possible World Semantics\n  as\
    \ ASP.   We propose an approximation of the Possible Worlds Semantics (PWS) for\
    \ action\nplanning. A corresponding planning system is implemented by a transformation\
    \ of\nthe action specification to an Answer-Set Program. A novelty is support\
    \ for\npostdiction wrt. (a) the plan existence problem in our framework can be\
    \ solved\nin NP, as compared to $\\Sigma_2^P$ for non-approximated PWS of Baral(2000);\
    \ and\n(b) the planner generates optimal plans wrt. a minimal number of actions\
    \ in\n$\\Delta_2^P$. We demo the planning system with standard problems, and\n\
    illustrate its integration in a larger software framework for robot control in\n\
    a smart home.\n"
- source_sentence: 'Distributed Pharaoh System for Network Routing.   In this paper
    it is introduced a biobjective ant algorithm for constructing

    low cost routing networks. The new algorithm is called the Distributed Pharaoh

    System (DPS). DPS is based on AntNet algorithm. The algorithm is using Pharaoh

    Ant System (PAS) with an extra-exploration phase and a ''no-entry'' condition
    in

    order to improve the solutions for the Low Cost Network Routing problem.

    Additionally it is used a cost model for overlay network construction that

    includes network traffic demands. The Pharaoh ants (Monomorium pharaonis)

    includes negative pheromones with signals concentrated at decision points where

    trails fork. The negative pheromones may complement positive pheromone or could

    help ants to escape from an unnecessarily long route to food that is being

    reinforced by attractive signals. Numerical experiments were made for a random

    10-node network. The average node degree of the network tested was 4.0. The

    results are encouraging. The algorithm converges to the shortest path while

    converging on a low cost overlay routing network topology.

    '
  sentences:
  - "Orthogonal multifilters image processing of astronomical images from\n  scanned\
    \ photographic plates.   In this paper orthogonal multifilters for astronomical\
    \ image processing are\npresented. We obtained new orthogonal multifilters based\
    \ on the orthogonal\nwavelet of Haar and Daubechies. Recently, multiwavelets have\
    \ been introduced as\na more powerful multiscale analysis tool. It adds several\
    \ degrees of freedom in\nmultifilter design and makes it possible to have several\
    \ useful properties such\nas symmetry, orthogonality, short support, and a higher\
    \ number of vanishing\nmoments simultaneously. Multifilter decomposition of scanned\
    \ photographic\nplates with astronomical images is made.\n"
  - "Unsupervised adaptation of brain machine interface decoders.   The performance\
    \ of neural decoders can degrade over time due to\nnonstationarities in the relationship\
    \ between neuronal activity and behavior.\nIn this case, brain-machine interfaces\
    \ (BMI) require adaptation of their\ndecoders to maintain high performance across\
    \ time. One way to achieve this is\nby use of periodical calibration phases, during\
    \ which the BMI system (or an\nexternal human demonstrator) instructs the user\
    \ to perform certain movements or\nbehaviors. This approach has two disadvantages:\
    \ (i) calibration phases\ninterrupt the autonomous operation of the BMI and (ii)\
    \ between two calibration\nphases the BMI performance might not be stable but\
    \ continuously decrease. A\nbetter alternative would be that the BMI decoder is\
    \ able to continuously adapt\nin an unsupervised manner during autonomous BMI\
    \ operation, i.e. without knowing\nthe movement intentions of the user.\n  In\
    \ the present article, we present an efficient method for such unsupervised\n\
    training of BMI systems for continuous movement control. The proposed method\n\
    utilizes a cost function derived from neuronal recordings, which guides a\nlearning\
    \ algorithm to evaluate the decoding parameters. We verify the\nperformance of\
    \ our adaptive method by simulating a BMI user with an optimal\nfeedback control\
    \ model and its interaction with our adaptive BMI decoder. The\nsimulation results\
    \ show that the cost function and the algorithm yield fast and\nprecise trajectories\
    \ towards targets at random orientations on a 2-dimensional\ncomputer screen.\
    \ For initially unknown and non-stationary tuning parameters,\nour unsupervised\
    \ method is still able to generate precise trajectories and to\nkeep its performance\
    \ stable in the long term. The algorithm can optionally work\nalso with neuronal\
    \ error signals instead or in conjunction with the proposed\nunsupervised adaptation.\n"
  - "Approximate inference on planar graphs using Loop Calculus and Belief\n  Propagation.\
    \   We introduce novel results for approximate inference on planar graphical\n\
    models using the loop calculus framework. The loop calculus (Chertkov and\nChernyak,\
    \ 2006) allows to express the exact partition function of a graphical\nmodel as\
    \ a finite sum of terms that can be evaluated once the belief\npropagation (BP)\
    \ solution is known. In general, full summation over all\ncorrection terms is\
    \ intractable. We develop an algorithm for the approach\npresented in (Certkov\
    \ et al., 2008) which represents an efficient truncation\nscheme on planar graphs\
    \ and a new representation of the series in terms of\nPfaffians of matrices. We\
    \ analyze the performance of the algorithm for the\npartition function approximation\
    \ for models with binary variables and pairwise\ninteractions on grids and other\
    \ planar graphs. We study in detail both the loop\nseries and the equivalent Pfaffian\
    \ series and show that the first term of the\nPfaffian series for the general,\
    \ intractable planar model, can provide very\naccurate approximations. The algorithm\
    \ outperforms previous truncation schemes\nof the loop series and is competitive\
    \ with other state-of-the-art methods for\napproximate inference.\n"
- source_sentence: 'Large-Sample Learning of Bayesian Networks is NP-Hard.   In this
    paper, we provide new complexity results for algorithms that learn

    discrete-variable Bayesian networks from data. Our results apply whenever the

    learning algorithm uses a scoring criterion that favors the simplest model able

    to represent the generative distribution exactly. Our results therefore hold

    whenever the learning algorithm uses a consistent scoring criterion and is

    applied to a sufficiently large dataset. We show that identifying high-scoring

    structures is hard, even when we are given an independence oracle, an inference

    oracle, and/or an information oracle. Our negative results also apply to the

    learning of discrete-variable Bayesian networks in which each node has at most

    k parents, for all k > 3.

    '
  sentences:
  - "Uncovering protein interaction in abstracts and text using a novel\n  linear\
    \ model and word proximity networks.   We participated in three of the protein-protein\
    \ interaction subtasks of the\nSecond BioCreative Challenge: classification of\
    \ abstracts relevant for\nprotein-protein interaction (IAS), discovery of protein\
    \ pairs (IPS) and text\npassages characterizing protein interaction (ISS) in full\
    \ text documents. We\napproached the abstract classification task with a novel,\
    \ lightweight linear\nmodel inspired by spam-detection techniques, as well as\
    \ an uncertainty-based\nintegration scheme. We also used a Support Vector Machine\
    \ and the Singular\nValue Decomposition on the same features for comparison purposes.\
    \ Our approach\nto the full text subtasks (protein pair and passage identification)\
    \ includes a\nfeature expansion method based on word-proximity networks. Our approach\
    \ to the\nabstract classification task (IAS) was among the top submissions for\
    \ this task\nin terms of the measures of performance used in the challenge evaluation\n\
    (accuracy, F-score and AUC). We also report on a web-tool we produced using our\n\
    approach: the Protein Interaction Abstract Relevance Evaluator (PIARE). Our\n\
    approach to the full text tasks resulted in one of the highest recall rates as\n\
    well as mean reciprocal rank of correct passages. Our approach to abstract\nclassification\
    \ shows that a simple linear model, using relatively few features,\nis capable\
    \ of generalizing and uncovering the conceptual nature of\nprotein-protein interaction\
    \ from the bibliome. Since the novel approach is\nbased on a very lightweight\
    \ linear model, it can be easily ported and applied\nto similar problems. In full\
    \ text problems, the expansion of word features with\nword-proximity networks\
    \ is shown to be useful, though the need for some\nimprovements is discussed.\n"
  - 'Identifying Dynamic Sequential Plans.   We address the problem of identifying
    dynamic sequential plans in the

    framework of causal Bayesian networks, and show that the problem is reduced to

    identifying causal effects, for which there are complete identi cation

    algorithms available in the literature.

    '
  - 'Large-Sample Learning of Bayesian Networks is NP-Hard.   In this paper, we provide
    new complexity results for algorithms that learn

    discrete-variable Bayesian networks from data. Our results apply whenever the

    learning algorithm uses a scoring criterion that favors the simplest model able

    to represent the generative distribution exactly. Our results therefore hold

    whenever the learning algorithm uses a consistent scoring criterion and is

    applied to a sufficiently large dataset. We show that identifying high-scoring

    structures is hard, even when we are given an independence oracle, an inference

    oracle, and/or an information oracle. Our negative results also apply to the

    learning of discrete-variable Bayesian networks in which each node has at most

    k parents, for all k > 3.

    '
pipeline_tag: sentence-similarity
library_name: sentence-transformers
---

# SentenceTransformer based on sentence-transformers/all-MiniLM-L6-v2

This is a [sentence-transformers](https://www.SBERT.net) model finetuned from [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2). It maps sentences & paragraphs to a 384-dimensional dense vector space and can be used for semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more.

## Model Details

### Model Description
- **Model Type:** Sentence Transformer
- **Base model:** [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) <!-- at revision c9745ed1d9f207416be6d2e6f8de32d1f16199bf -->
- **Maximum Sequence Length:** 256 tokens
- **Output Dimensionality:** 384 dimensions
- **Similarity Function:** Cosine Similarity
<!-- - **Training Dataset:** Unknown -->
<!-- - **Language:** Unknown -->
<!-- - **License:** Unknown -->

### Model Sources

- **Documentation:** [Sentence Transformers Documentation](https://sbert.net)
- **Repository:** [Sentence Transformers on GitHub](https://github.com/UKPLab/sentence-transformers)
- **Hugging Face:** [Sentence Transformers on Hugging Face](https://huggingface.co/models?library=sentence-transformers)

### Full Model Architecture

```
SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
)
```

## Usage

### Direct Usage (Sentence Transformers)

First install the Sentence Transformers library:

```bash
pip install -U sentence-transformers
```

Then you can load this model and run inference.
```python
from sentence_transformers import SentenceTransformer

# Download from the 🤗 Hub
model = SentenceTransformer("sentence_transformers_model_id")
# Run inference
sentences = [
    'Large-Sample Learning of Bayesian Networks is NP-Hard.   In this paper, we provide new complexity results for algorithms that learn\ndiscrete-variable Bayesian networks from data. Our results apply whenever the\nlearning algorithm uses a scoring criterion that favors the simplest model able\nto represent the generative distribution exactly. Our results therefore hold\nwhenever the learning algorithm uses a consistent scoring criterion and is\napplied to a sufficiently large dataset. We show that identifying high-scoring\nstructures is hard, even when we are given an independence oracle, an inference\noracle, and/or an information oracle. Our negative results also apply to the\nlearning of discrete-variable Bayesian networks in which each node has at most\nk parents, for all k > 3.\n',
    'Large-Sample Learning of Bayesian Networks is NP-Hard.   In this paper, we provide new complexity results for algorithms that learn\ndiscrete-variable Bayesian networks from data. Our results apply whenever the\nlearning algorithm uses a scoring criterion that favors the simplest model able\nto represent the generative distribution exactly. Our results therefore hold\nwhenever the learning algorithm uses a consistent scoring criterion and is\napplied to a sufficiently large dataset. We show that identifying high-scoring\nstructures is hard, even when we are given an independence oracle, an inference\noracle, and/or an information oracle. Our negative results also apply to the\nlearning of discrete-variable Bayesian networks in which each node has at most\nk parents, for all k > 3.\n',
    'Uncovering protein interaction in abstracts and text using a novel\n  linear model and word proximity networks.   We participated in three of the protein-protein interaction subtasks of the\nSecond BioCreative Challenge: classification of abstracts relevant for\nprotein-protein interaction (IAS), discovery of protein pairs (IPS) and text\npassages characterizing protein interaction (ISS) in full text documents. We\napproached the abstract classification task with a novel, lightweight linear\nmodel inspired by spam-detection techniques, as well as an uncertainty-based\nintegration scheme. We also used a Support Vector Machine and the Singular\nValue Decomposition on the same features for comparison purposes. Our approach\nto the full text subtasks (protein pair and passage identification) includes a\nfeature expansion method based on word-proximity networks. Our approach to the\nabstract classification task (IAS) was among the top submissions for this task\nin terms of the measures of performance used in the challenge evaluation\n(accuracy, F-score and AUC). We also report on a web-tool we produced using our\napproach: the Protein Interaction Abstract Relevance Evaluator (PIARE). Our\napproach to the full text tasks resulted in one of the highest recall rates as\nwell as mean reciprocal rank of correct passages. Our approach to abstract\nclassification shows that a simple linear model, using relatively few features,\nis capable of generalizing and uncovering the conceptual nature of\nprotein-protein interaction from the bibliome. Since the novel approach is\nbased on a very lightweight linear model, it can be easily ported and applied\nto similar problems. In full text problems, the expansion of word features with\nword-proximity networks is shown to be useful, though the need for some\nimprovements is discussed.\n',
]
embeddings = model.encode(sentences)
print(embeddings.shape)
# [3, 384]

# Get the similarity scores for the embeddings
similarities = model.similarity(embeddings, embeddings)
print(similarities.shape)
# [3, 3]
```

<!--
### Direct Usage (Transformers)

<details><summary>Click to see the direct usage in Transformers</summary>

</details>
-->

<!--
### Downstream Usage (Sentence Transformers)

You can finetune this model on your own dataset.

<details><summary>Click to expand</summary>

</details>
-->

<!--
### Out-of-Scope Use

*List how the model may foreseeably be misused and address what users ought not to do with the model.*
-->

<!--
## Bias, Risks and Limitations

*What are the known or foreseeable issues stemming from this model? You could also flag here known failure cases or weaknesses of the model.*
-->

<!--
### Recommendations

*What are recommendations with respect to the foreseeable issues? For example, filtering explicit content.*
-->

## Training Details

### Training Dataset

#### Unnamed Dataset

* Size: 14,998 training samples
* Columns: <code>sentence_0</code>, <code>sentence_1</code>, and <code>label</code>
* Approximate statistics based on the first 1000 samples:
  |         | sentence_0                                                                           | sentence_1                                                                           | label                                                          |
  |:--------|:-------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------|:---------------------------------------------------------------|
  | type    | string                                                                               | string                                                                               | float                                                          |
  | details | <ul><li>min: 24 tokens</li><li>mean: 185.87 tokens</li><li>max: 256 tokens</li></ul> | <ul><li>min: 24 tokens</li><li>mean: 183.81 tokens</li><li>max: 256 tokens</li></ul> | <ul><li>min: 0.0</li><li>mean: 0.67</li><li>max: 1.0</li></ul> |
* Samples:
  | sentence_0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | sentence_1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | label            |
  |:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------|
  | <code>Mining to Compact CNF Propositional Formulae.   In this paper, we propose a first application of data mining techniques to<br>propositional satisfiability. Our proposed Mining4SAT approach aims to discover<br>and to exploit hidden structural knowledge for reducing the size of<br>propositional formulae in conjunctive normal form (CNF). Mining4SAT combines<br>both frequent itemset mining techniques and Tseitin's encoding for a compact<br>representation of CNF formulae. The experiments of our Mining4SAT approach show<br>interesting reductions of the sizes of many application instances taken from<br>the last SAT competitions.<br></code>                                                                                                                                                                               | <code>Mining to Compact CNF Propositional Formulae.   In this paper, we propose a first application of data mining techniques to<br>propositional satisfiability. Our proposed Mining4SAT approach aims to discover<br>and to exploit hidden structural knowledge for reducing the size of<br>propositional formulae in conjunctive normal form (CNF). Mining4SAT combines<br>both frequent itemset mining techniques and Tseitin's encoding for a compact<br>representation of CNF formulae. The experiments of our Mining4SAT approach show<br>interesting reductions of the sizes of many application instances taken from<br>the last SAT competitions.<br></code>                     | <code>1.0</code> |
  | <code>Quantum Annealing for Clustering.   This paper studies quantum annealing (QA) for clustering, which can be seen<br>as an extension of simulated annealing (SA). We derive a QA algorithm for<br>clustering and propose an annealing schedule, which is crucial in practice.<br>Experiments show the proposed QA algorithm finds better clustering assignments<br>than SA. Furthermore, QA is as easy as SA to implement.<br></code>                                                                                                                                                                                                                                                                                                                                                                                                            | <code>Quantum Annealing for Clustering.   This paper studies quantum annealing (QA) for clustering, which can be seen<br>as an extension of simulated annealing (SA). We derive a QA algorithm for<br>clustering and propose an annealing schedule, which is crucial in practice.<br>Experiments show the proposed QA algorithm finds better clustering assignments<br>than SA. Furthermore, QA is as easy as SA to implement.<br></code>                                                                                                                                                                                                                                                  | <code>1.0</code> |
  | <code>Graph Regularized Nonnegative Matrix Factorization for Hyperspectral<br>  Data Unmixing.   Spectral unmixing is an important tool in hyperspectral data analysis for<br>estimating endmembers and abundance fractions in a mixed pixel. This paper<br>examines the applicability of a recently developed algorithm called graph<br>regularized nonnegative matrix factorization (GNMF) for this aim. The proposed<br>approach exploits the intrinsic geometrical structure of the data besides<br>considering positivity and full additivity constraints. Simulated data based on<br>the measured spectral signatures, is used for evaluating the proposed<br>algorithm. Results in terms of abundance angle distance (AAD) and spectral<br>angle distance (SAD) show that this method can effectively unmix hyperspectral<br>data.<br></code> | <code>Can We Learn to Beat the Best Stock.   A novel algorithm for actively trading stocks is presented. While traditional<br>expert advice and "universal" algorithms (as well as standard technical trading<br>heuristics) attempt to predict winners or trends, our approach relies on<br>predictable statistical relations between all pairs of stocks in the market.<br>Our empirical results on historical markets provide strong evidence that this<br>type of technical trading can "beat the market" and moreover, can beat the best<br>stock in the market. In doing so we utilize a new idea for smoothing critical<br>parameters in the context of expert learning.<br></code> | <code>0.0</code> |
* Loss: [<code>CosineSimilarityLoss</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#cosinesimilarityloss) with these parameters:
  ```json
  {
      "loss_fct": "torch.nn.modules.loss.MSELoss"
  }
  ```

### Training Hyperparameters
#### Non-Default Hyperparameters

- `per_device_train_batch_size`: 16
- `per_device_eval_batch_size`: 16
- `num_train_epochs`: 1
- `multi_dataset_batch_sampler`: round_robin

#### All Hyperparameters
<details><summary>Click to expand</summary>

- `overwrite_output_dir`: False
- `do_predict`: False
- `eval_strategy`: no
- `prediction_loss_only`: True
- `per_device_train_batch_size`: 16
- `per_device_eval_batch_size`: 16
- `per_gpu_train_batch_size`: None
- `per_gpu_eval_batch_size`: None
- `gradient_accumulation_steps`: 1
- `eval_accumulation_steps`: None
- `torch_empty_cache_steps`: None
- `learning_rate`: 5e-05
- `weight_decay`: 0.0
- `adam_beta1`: 0.9
- `adam_beta2`: 0.999
- `adam_epsilon`: 1e-08
- `max_grad_norm`: 1
- `num_train_epochs`: 1
- `max_steps`: -1
- `lr_scheduler_type`: linear
- `lr_scheduler_kwargs`: {}
- `warmup_ratio`: 0.0
- `warmup_steps`: 0
- `log_level`: passive
- `log_level_replica`: warning
- `log_on_each_node`: True
- `logging_nan_inf_filter`: True
- `save_safetensors`: True
- `save_on_each_node`: False
- `save_only_model`: False
- `restore_callback_states_from_checkpoint`: False
- `no_cuda`: False
- `use_cpu`: False
- `use_mps_device`: False
- `seed`: 42
- `data_seed`: None
- `jit_mode_eval`: False
- `use_ipex`: False
- `bf16`: False
- `fp16`: False
- `fp16_opt_level`: O1
- `half_precision_backend`: auto
- `bf16_full_eval`: False
- `fp16_full_eval`: False
- `tf32`: None
- `local_rank`: 0
- `ddp_backend`: None
- `tpu_num_cores`: None
- `tpu_metrics_debug`: False
- `debug`: []
- `dataloader_drop_last`: False
- `dataloader_num_workers`: 0
- `dataloader_prefetch_factor`: None
- `past_index`: -1
- `disable_tqdm`: False
- `remove_unused_columns`: True
- `label_names`: None
- `load_best_model_at_end`: False
- `ignore_data_skip`: False
- `fsdp`: []
- `fsdp_min_num_params`: 0
- `fsdp_config`: {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}
- `tp_size`: 0
- `fsdp_transformer_layer_cls_to_wrap`: None
- `accelerator_config`: {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}
- `deepspeed`: None
- `label_smoothing_factor`: 0.0
- `optim`: adamw_torch
- `optim_args`: None
- `adafactor`: False
- `group_by_length`: False
- `length_column_name`: length
- `ddp_find_unused_parameters`: None
- `ddp_bucket_cap_mb`: None
- `ddp_broadcast_buffers`: False
- `dataloader_pin_memory`: True
- `dataloader_persistent_workers`: False
- `skip_memory_metrics`: True
- `use_legacy_prediction_loop`: False
- `push_to_hub`: False
- `resume_from_checkpoint`: None
- `hub_model_id`: None
- `hub_strategy`: every_save
- `hub_private_repo`: None
- `hub_always_push`: False
- `gradient_checkpointing`: False
- `gradient_checkpointing_kwargs`: None
- `include_inputs_for_metrics`: False
- `include_for_metrics`: []
- `eval_do_concat_batches`: True
- `fp16_backend`: auto
- `push_to_hub_model_id`: None
- `push_to_hub_organization`: None
- `mp_parameters`: 
- `auto_find_batch_size`: False
- `full_determinism`: False
- `torchdynamo`: None
- `ray_scope`: last
- `ddp_timeout`: 1800
- `torch_compile`: False
- `torch_compile_backend`: None
- `torch_compile_mode`: None
- `include_tokens_per_second`: False
- `include_num_input_tokens_seen`: False
- `neftune_noise_alpha`: None
- `optim_target_modules`: None
- `batch_eval_metrics`: False
- `eval_on_start`: False
- `use_liger_kernel`: False
- `eval_use_gather_object`: False
- `average_tokens_across_devices`: False
- `prompts`: None
- `batch_sampler`: batch_sampler
- `multi_dataset_batch_sampler`: round_robin

</details>

### Training Logs
| Epoch  | Step | Training Loss |
|:------:|:----:|:-------------:|
| 0.5330 | 500  | 0.0044        |


### Framework Versions
- Python: 3.10.9
- Sentence Transformers: 4.1.0
- Transformers: 4.51.3
- PyTorch: 2.7.0+cpu
- Accelerate: 1.6.0
- Datasets: 3.5.1
- Tokenizers: 0.21.1

## Citation

### BibTeX

#### Sentence Transformers
```bibtex
@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/1908.10084",
}
```

<!--
## Glossary

*Clearly define terms in order to be accessible across audiences.*
-->

<!--
## Model Card Authors

*Lists the people who create the model card, providing recognition and accountability for the detailed work that goes into its construction.*
-->

<!--
## Model Card Contact

*Provides a way for people who have updates to the Model Card, suggestions, or questions, to contact the Model Card authors.*
-->